{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Imports \n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import StaleElementReferenceException, TimeoutException, NoSuchElementException\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Main code for bike data\n",
    "    scrapping till the specs step\"\"\" \n",
    "# Note - might raise exceptions when a popup appears in between the scappring which leads to a stopage on a point, due to this navigation approach \n",
    "# Navigate back to the main bike page \n",
    "# driver.execute_script(\"window.history.go(-2)\")\" as \"-2\" defines the procedure to go 2 pages back in history point.\n",
    "# We can bypass it by simply closing the pop-up then manually moving towards the main bikelist page it will retry automatically. \n",
    "\n",
    "#perfect one ,\"ran successfully for the whole list \"\n",
    "\n",
    "def find_element_with_retry(driver, by, value, retries=3, delay=2):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            element = WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((by, value))\n",
    "            )\n",
    "            return element\n",
    "        except (TimeoutException, StaleElementReferenceException) as e:\n",
    "            print(f\"Error finding element: {str(e)}. Retrying...\")\n",
    "            sleep(delay)\n",
    "    raise Exception(f\"Unable to find element {value} after {retries} retries.\")\n",
    "\n",
    "def select_dropdown_option(driver):\n",
    "    dropdown_option_xpath = '//*[@id=\"rf01\"]/header/div[1]/div/div/div[2]/div/div/form/div/div/div/ul/li'\n",
    "    try:\n",
    "        dropdown_options = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, dropdown_option_xpath))\n",
    "        )\n",
    "        if dropdown_options:\n",
    "            dropdown_options[0].click()  # Clicking the first option\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error selecting dropdown option: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def scrape_bike_details(search_query, driver):\n",
    "    bike_details_list = []\n",
    "    try:\n",
    "        # Find the search text input element\n",
    "        search_text_input = find_element_with_retry(driver, By.ID, 'cardekhosearchtext')\n",
    "\n",
    "        # Input the search query into the search text input\n",
    "        search_text_input.clear()\n",
    "        search_text_input.send_keys(search_query)\n",
    "\n",
    "        # Wait for a while to let the dropdown options appear\n",
    "        sleep(1)\n",
    "\n",
    "        # Re-find the dropdown options after waiting\n",
    "        if not select_dropdown_option(driver):\n",
    "            print(f\"Failed to select dropdown option for query: {search_query}\")\n",
    "            return bike_details_list\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        bike_list_element = find_element_with_retry(driver, By.CLASS_NAME, 'bikelist')\n",
    "\n",
    "        for index in range(len(bike_list_element.find_elements(By.TAG_NAME, 'li'))):\n",
    "            try:\n",
    "                # Re-find the bike list element to avoid stale element reference\n",
    "                bike_list_element = find_element_with_retry(driver, By.CLASS_NAME, 'bikelist')\n",
    "                \n",
    "                item = bike_list_element.find_elements(By.TAG_NAME, 'li')[index]\n",
    "                x_element = item.find_element(By.TAG_NAME, 'h3').find_element(By.TAG_NAME, 'a')\n",
    "                link = x_element.get_attribute('href')\n",
    "                \n",
    "                # Move to the element to ensure it's clickable\n",
    "                ActionChains(driver).move_to_element(x_element).click().perform()\n",
    "\n",
    "                # Additional scraping from the new page (e.g., bike name, price, etc.)\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//h1')))\n",
    "                bike_name = driver.find_element(By.XPATH, '//h1').text\n",
    "\n",
    "                # Fetch the price\n",
    "                bike_price_element = driver.find_element(By.XPATH, '//div[@class=\"price\"]')\n",
    "                bike_price_text = bike_price_element.text.split('Rs.')[-1].split('*')[0].strip()\n",
    "\n",
    "                # Check if the price contains a range\n",
    "                if '-' in bike_price_text:\n",
    "                    min_price, max_price = bike_price_text.split('-')\n",
    "                    bike_price = f\"{min_price.strip()} Lakh - {max_price.strip()}\"\n",
    "                else:\n",
    "                    bike_price = f\"{bike_price_text}\"\n",
    "\n",
    "                # Click on the \"Specs\" button to navigate to the specifications page\n",
    "                specs_button = find_element_with_retry(driver, By.XPATH, '//a[contains(text(), \"Specs\")]')\n",
    "\n",
    "                # Move to the element to ensure it's clickable\n",
    "                ActionChains(driver).move_to_element(specs_button).click().perform()\n",
    "\n",
    "                # Wait for the technical specifications section to be present\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'technicalSpecs')))\n",
    "\n",
    "                # Find the technical specifications block\n",
    "                tech_specs_block = driver.find_element(By.ID, 'technicalSpecs')\n",
    "\n",
    "                # Find all h3 elements within the technical specifications block\n",
    "                section_titles = tech_specs_block.find_elements(By.TAG_NAME, 'h3')\n",
    "\n",
    "                for title_element in section_titles:\n",
    "                    section_title = title_element.text\n",
    "                    section_data = {}\n",
    "\n",
    "                    # Find the corresponding table for each section\n",
    "                    table = title_element.find_element(By.XPATH, './following-sibling::table[1]')\n",
    "\n",
    "                    # Find all rows in the table\n",
    "                    rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "                    for row in rows:\n",
    "                        columns = row.find_elements(By.TAG_NAME, 'td')\n",
    "                        if len(columns) == 2:\n",
    "                            feature = columns[0].text\n",
    "                            value = columns[1].text\n",
    "                            section_data[feature] = value\n",
    "\n",
    "                    # Add the section data to the bike details list\n",
    "                    bike_details_list.append({\n",
    "                        'Search Query': search_query,\n",
    "                        'Bike Name': bike_name,\n",
    "                        'Price': bike_price,\n",
    "                        'Section Title': section_title,\n",
    "                        'Section Data': section_data\n",
    "                    })\n",
    "\n",
    "                # Navigate back to the main bike page\n",
    "                driver.execute_script(\"window.history.go(-2)\")\n",
    "                \n",
    "                # Wait for the bike list to load again\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'bikelist')))\n",
    "                \n",
    "            except NoSuchElementException as ne:\n",
    "                print(f\"No <h3> tag found for item {search_query}. Skipping...\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing item {search_query}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except NoSuchElementException as ne:\n",
    "        print(f\"No element found: {ne}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "    return bike_details_list\n",
    "\n",
    "try:\n",
    "    bike_details_list = []\n",
    "\n",
    "    with webdriver.Chrome() as driver:\n",
    "        driver.get('https://www.bikedekho.com')\n",
    "\n",
    "        search_queries = ['Bajaj bike', 'TVS bike', 'Hero bike', 'Royal Enfield bike','Honda bike', 'Yamaha bike', 'Suzuki bike','KTM bike',\n",
    "            'Jawa bike', 'Benelli bike', 'Harley bike', 'Triumph bike',\n",
    "            'Ducati bike', 'BMW bike', 'Kawasaki bike', 'Husqvarna bike',\n",
    "            'Revolt bike', 'Ather bike','Ola bike','Aprilia Bikes']\n",
    "        \n",
    "        for query in search_queries:\n",
    "            result = scrape_bike_details(query, driver)\n",
    "            bike_details_list.extend(result)\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    dv = pd.DataFrame(bike_details_list)\n",
    "    \n",
    "    # # Save the DataFrame to a CSV file\n",
    "    # dfz.to_csv('bike_details.csv', index=False)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df6 = df6.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### working for specs page link extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working for link extraction \n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "\n",
    "def find_element_with_retry(driver, by, value, retries=3, delay=2):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            element = WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((by, value))\n",
    "            )\n",
    "            return element\n",
    "        except (TimeoutException, NoSuchElementException) as e:\n",
    "            print(f\"Error finding element: {str(e)}. Retrying...\")\n",
    "            sleep(delay)\n",
    "    raise Exception(f\"Unable to find element {value} after {retries} retries.\")\n",
    "\n",
    "def select_dropdown_option(driver):\n",
    "    dropdown_option_xpath = '//*[@id=\"rf01\"]/header/div[1]/div/div/div[2]/div/div/form/div/div/div/ul/li'\n",
    "    try:\n",
    "        dropdown_options = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.XPATH, dropdown_option_xpath))\n",
    "        )\n",
    "        if dropdown_options:\n",
    "            dropdown_options[0].click()  # Clicking the first option\n",
    "            return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error selecting dropdown option: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "def scrape_bike_specs_links(search_query, driver):\n",
    "    bike_links_list = []\n",
    "    try:\n",
    "        # Find the search text input element\n",
    "        search_text_input = find_element_with_retry(driver, By.ID, 'cardekhosearchtext')\n",
    "\n",
    "        # Input the search query into the search text input\n",
    "        search_text_input.clear()\n",
    "        search_text_input.send_keys(search_query)\n",
    "\n",
    "        # Wait for a while to let the dropdown options appear\n",
    "        sleep(1)\n",
    "\n",
    "        # Re-find the dropdown options after waiting\n",
    "        if not select_dropdown_option(driver):\n",
    "            print(f\"Failed to select dropdown option for query: {search_query}\")\n",
    "            return bike_links_list\n",
    "\n",
    "        sleep(1)\n",
    "\n",
    "        bike_list_element = find_element_with_retry(driver, By.CLASS_NAME, 'bikelist')\n",
    "\n",
    "        for index in range(len(bike_list_element.find_elements(By.TAG_NAME, 'li'))):\n",
    "            try:\n",
    "                # Re-find the bike list element to avoid stale element reference\n",
    "                bike_list_element = find_element_with_retry(driver, By.CLASS_NAME, 'bikelist')\n",
    "                \n",
    "                item = bike_list_element.find_elements(By.TAG_NAME, 'li')[index]\n",
    "                x_element = item.find_element(By.TAG_NAME, 'h3').find_element(By.TAG_NAME, 'a')\n",
    "                link = x_element.get_attribute('href')\n",
    "\n",
    "                # Navigate to the bike's page\n",
    "                driver.get(link)\n",
    "                \n",
    "                # Find the \"Specs\" link\n",
    "                specs_link_element = find_element_with_retry(driver, By.XPATH, '//a[contains(@title, \"Specifications\")]')\n",
    "                specs_link = specs_link_element.get_attribute('href')\n",
    "                \n",
    "                bike_links_list.append({'Search Query': search_query, 'Specs Link': specs_link})\n",
    "                print(f\"Scraped Specs link for {search_query}: {specs_link}\")\n",
    "\n",
    "                # Navigate back to the main bike page\n",
    "                driver.execute_script(\"window.history.go(-1)\")\n",
    "                \n",
    "                # Wait for the bike list to load again\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'bikelist')))\n",
    "                \n",
    "            except NoSuchElementException as ne:\n",
    "                print(f\"No <h3> tag found for item {search_query}. Skipping...\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing item {search_query}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    except NoSuchElementException as ne:\n",
    "        print(f\"No element found: {ne}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "    return bike_links_list\n",
    "\n",
    "try:\n",
    "    bike_links_list = []\n",
    "\n",
    "    with webdriver.Chrome() as driver:\n",
    "        driver.get('https://www.bikedekho.com')\n",
    "        search_queries = ['Hero bike','Aprilia Bikes']\n",
    "        # search_queries = ['Bajaj bike', 'TVS bike', 'Hero bike', 'Royal Enfield bike','Honda bike', 'Yamaha bike', 'Suzuki bike','KTM bike',\n",
    "        #     'Jawa bike', 'Benelli bike', 'Harley bike', 'Triumph bike',\n",
    "        #     'Ducati bike', 'BMW bike', 'Kawasaki bike', 'Husqvarna bike',\n",
    "        #     'Revolt bike', 'Ather bike','Ola bike','Aprilia Bikes']\n",
    "        \n",
    "        for query in search_queries:\n",
    "            result = scrape_bike_specs_links(query, driver)\n",
    "            bike_links_list.extend(result)\n",
    "\n",
    "    # Convert the list of dictionaries to a pandas DataFrame\n",
    "    dfi = pd.DataFrame(bike_links_list)\n",
    "    print(dfi)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = dfi[dfi.duplicated(subset=['Search Query', 'Specs Link'], keep=False)]\n",
    "print(duplicates)\n",
    "\n",
    "# # Convert the list of dictionaries to a pandas DataFrame\n",
    "# # dfv = pd.DataFrame(bike_links_list)\n",
    "\n",
    "# Drop duplicate rows based on 'Search Query' and 'Specs Link'\n",
    "# dfi = dfi.drop_duplicates(subset=['Search Query', 'Specs Link'])\n",
    "\n",
    "# print(dfi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert the list of dictionaries to a pandas DataFrame\n",
    "# # dfv = pd.DataFrame(bike_links_list)\n",
    "\n",
    "# # Drop duplicate rows based on 'Search Query' and 'Specs Link'\n",
    "# df6 = df6.drop_duplicates(subset=['Search Query', 'Specs Link'])\n",
    "\n",
    "# print(df6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfi.to_csv('bike_spec_page_link_hero_aprillia.csv', index=False)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Read the CSV files into pandas DataFrames\n",
    "# df8 = pd.read_csv('bike_spec_page_link_hero_aprillia.csv')\n",
    "# df9 = pd.read_csv('bike_spec_page_linksall.csv')\n",
    "\n",
    "# # Concatenate the DataFrames\n",
    "# df_concatenated = pd.concat([df8, df9], ignore_index=True)\n",
    "\n",
    "# # # Optionally, reset the index\n",
    "# df_concatenated = df_concatenated.reset_index(drop=True)\n",
    "\n",
    "# # # Save the concatenated DataFrame to a new CSV file\n",
    "# df_concatenated.to_csv('concatenated_file.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dffinal = pd.read_csv('/Users/atharv/Desktop/Bikes_scrapping/notebooks/concatenated_file.csv')\n",
    "duplicates = dffinal[dffinal.duplicated(subset=['Search Query', 'Specs Link'], keep=False)]\n",
    "print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # dfz['Bike Name'].nunique()\n",
    "\n",
    "# # print(dfz.isnull().sum())\n",
    "\n",
    "# # dfz.to_csv('bike_specZ.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['Yamaha MT 15 V2', 'Yamaha R15 V4', 'Yamaha R15S',\n",
    "#        'Yamaha FZS-FI V3', 'Yamaha RayZR 125 Fi Hybrid',\n",
    "#        'Yamaha Aerox 155', 'Yamaha FZ-FI V3',\n",
    "#        'Yamaha Fascino 125 Fi Hybrid', 'Yamaha FZS-FI V4', 'Yamaha FZ X',\n",
    "#        'Yamaha R3', 'Yamaha MT-03', 'Yamaha FZ 25', 'Yamaha FZS 25'].sort"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for specs extraction only using specs page direct link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working fine for specz extraction using specs page link\n",
    "\n",
    "# Initialize the browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Navigate to the URL\n",
    "driver.get('https://www.bikedekho.com/ola-electric/s1-pro/specifications')\n",
    "\n",
    "\n",
    "try:\n",
    "    # Wait for the technical specifications section to be present\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'technicalSpecs')))\n",
    "\n",
    "    # Find the technical specifications block\n",
    "    tech_specs_block = driver.find_element(By.ID, 'technicalSpecs')\n",
    "\n",
    "    # Find all h3 elements within the technical specifications block\n",
    "    section_titles = tech_specs_block.find_elements(By.TAG_NAME, 'h3')\n",
    "\n",
    "    for title_element in section_titles:\n",
    "        section_title = title_element.text\n",
    "        print(f\"=== {section_title} ===\")\n",
    "        \n",
    "        # Find the corresponding table for each section\n",
    "        table = title_element.find_element(By.XPATH, './following-sibling::table[1]')\n",
    "        \n",
    "        # Find all rows in the table\n",
    "        rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "        \n",
    "        for row in rows:\n",
    "            columns = row.find_elements(By.TAG_NAME, 'td')\n",
    "            if len(columns) == 2:\n",
    "                feature = columns[0].text\n",
    "                value = columns[1].text\n",
    "                print(f\"{feature}: {value}\")\n",
    "        \n",
    "        print(\"\\n\")  # Separate sections with a newline\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method for specs extraction like -- *Features*,*Price*,*Bike name*,*Sections* -- , using specs page direct link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#woking for data extraction form links final one \n",
    "\n",
    "# Read the CSV file containing the links\n",
    "dflinks = pd.read_csv('/Users/atharv/Desktop/Bikes_scrapping/notebooks/concatenated_file.csv')\n",
    "\n",
    "# Initialize the browser\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "try:\n",
    "    bike_details_list = []\n",
    "\n",
    "    for link in dflinks['Specs Link']:\n",
    "        # Navigate to the URL\n",
    "        driver.get(link)\n",
    "\n",
    "        # Wait for the bike name to be present\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//h1')))\n",
    "\n",
    "        # Extract bike name\n",
    "        bike_name_full = driver.find_element(By.XPATH, '//h1').text\n",
    "        bike_name = bike_name_full.split(' of ')[-1].strip()\n",
    "\n",
    "        # Extract bike price\n",
    "        try:\n",
    "            bike_price_element = driver.find_element(By.XPATH, '//div[contains(@class, \"price\")]//i[contains(@class, \"icon-cd_R\")]//parent::span')\n",
    "            \n",
    "            if bike_price_element.is_displayed():\n",
    "                bike_price_text = bike_price_element.text.split('Rs.')[-1].split('*')[0].strip()\n",
    "                \n",
    "                print(f\"Bike Name: {bike_name}\")\n",
    "                print(f\"Price: {bike_price_text}\")\n",
    "                \n",
    "            else:\n",
    "                print(\"Price element is not visible\")\n",
    "                bike_price_text = \"N/A\"\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error extracting bike price: {e}\")\n",
    "            bike_price_text = \"N/A\"\n",
    "\n",
    "        bike_details = {\n",
    "            'Bike Name': bike_name,\n",
    "            'Price': bike_price_text,\n",
    "            'Specs': []\n",
    "        }\n",
    "\n",
    "        # Wait for the technical specifications section to be present\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.ID, 'technicalSpecs')))\n",
    "\n",
    "        # Find the technical specifications block\n",
    "        tech_specs_block = driver.find_element(By.ID, 'technicalSpecs')\n",
    "\n",
    "        # Find all h3 elements within the technical specifications block\n",
    "        section_titles = tech_specs_block.find_elements(By.TAG_NAME, 'h3')\n",
    "\n",
    "        for title_element in section_titles:\n",
    "            section_title = title_element.text\n",
    "\n",
    "            # Find the corresponding table for each section\n",
    "            table = title_element.find_element(By.XPATH, './following-sibling::table[1]')\n",
    "\n",
    "            # Find all rows in the table\n",
    "            rows = table.find_elements(By.TAG_NAME, 'tr')\n",
    "\n",
    "            for row in rows:\n",
    "                columns = row.find_elements(By.TAG_NAME, 'td')\n",
    "                if len(columns) == 2:\n",
    "                    feature = columns[0].text\n",
    "                    value = columns[1].text\n",
    "\n",
    "                    # Append the feature and value to the bike_details['Specs']\n",
    "                    bike_details['Specs'].append({\n",
    "                        'Feature': feature,\n",
    "                        'Value': value,\n",
    "                        'Section Title': section_title,\n",
    "                    })\n",
    "\n",
    "        # Append the bike_details to the bike_details_list\n",
    "        bike_details_list.append(bike_details)\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    driver.quit()\n",
    "\n",
    "# Create a dataframe from the bike_details_list\n",
    "df_bike_details = pd.DataFrame([\n",
    "    {'Bike Name': bike['Bike Name'], 'Price': bike['Price'], **spec}\n",
    "    for bike in bike_details_list\n",
    "    for spec in bike['Specs']\n",
    "])\n",
    "\n",
    "# Print the dataframe\n",
    "print(df_bike_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- df_bike_details.to_csv('bike_specall.csv', index=False) -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_bike_details.to_csv('bike_specall.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dffin = pd.read_csv('/Users/atharv/Desktop/Bikes_scrapping/notebooks/bike_specall.csv')\n",
    "# duplicates = dffin[dffin.duplicated(subset=['Bike Name','Price','Feature','Value','Section Title'], keep=False)]\n",
    "# print(duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#working fine for bike main page data fetching \n",
    "\n",
    "def find_element_with_retry(driver, by, value, retries=3, delay=2):\n",
    "    for _ in range(retries):\n",
    "        try:\n",
    "            element = WebDriverWait(driver, delay).until(\n",
    "                EC.presence_of_element_located((by, value))\n",
    "            )\n",
    "            return element\n",
    "        except (TimeoutException, StaleElementReferenceException) as e:\n",
    "            print(f\"Error finding element: {str(e)}. Retrying...\")\n",
    "            sleep(delay)\n",
    "    raise Exception(f\"Unable to find element {value} after {retries} retries.\")\n",
    "\n",
    "try:\n",
    "    # Initialize the browser\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    # Navigate to the URL\n",
    "    driver.get('https://www.bikedekho.com')\n",
    "\n",
    "    # List of search queries\n",
    "    search_queries = [\n",
    "        'ola bike', 'Bajaj bike', 'TVS bike', 'Royal Enfield bike',\n",
    "        'Honda bike', 'Yamaha bike', 'Suzuki bike', 'KTM bike',\n",
    "        'Jawa bike', 'Benelli bike', 'Harley bike', 'Triumph bike',\n",
    "        'Ducati bike', 'BMW bike', 'Kawasaki bike', 'Husqvarna bike',\n",
    "        'Revolt bike', 'Ather bike','Aprillia bike'\n",
    "    ]\n",
    "\n",
    "    bike_details = []\n",
    "\n",
    "    for search_query in search_queries:\n",
    "        # Find the search text input element\n",
    "        search_text_input = find_element_with_retry(driver, By.ID, 'cardekhosearchtext')\n",
    "\n",
    "        # Input the search query into the search text input\n",
    "        search_text_input.clear()\n",
    "        search_text_input.send_keys(search_query)\n",
    "\n",
    "        # Wait for a while to let the dropdown options appear\n",
    "        sleep(2)\n",
    "\n",
    "        # Re-find the dropdown options after waiting\n",
    "        dropdown_option_xpath = '//*[@id=\"rf01\"]/header/div[1]/div/div/div[2]/div/div/form/div/div/div/ul/li'\n",
    "        dropdown_options = find_element_with_retry(driver, By.XPATH, dropdown_option_xpath, retries=5, delay=2)\n",
    "\n",
    "        # Select the first option from the dropdown if available\n",
    "        if dropdown_options:\n",
    "            first_option = dropdown_options\n",
    "            driver.execute_script(\"arguments[0].scrollIntoView();\", first_option)\n",
    "            first_option.click()\n",
    "\n",
    "        sleep(2)\n",
    "        \n",
    "        bike_list_element = find_element_with_retry(driver, By.CLASS_NAME, 'bikelist')\n",
    "\n",
    "        for index in range(len(bike_list_element.find_elements(By.TAG_NAME, 'li'))):\n",
    "            try:\n",
    "                # Re-find the bike list element to avoid stale element reference\n",
    "                bike_list_element = find_element_with_retry(driver, By.CLASS_NAME, 'bikelist')\n",
    "                \n",
    "                item = bike_list_element.find_elements(By.TAG_NAME, 'li')[index]\n",
    "                x_element = item.find_element(By.TAG_NAME, 'h3').find_element(By.TAG_NAME, 'a')\n",
    "                link = x_element.get_attribute('href')\n",
    "                bike_details.append(link)\n",
    "                \n",
    "                # Click on the link to navigate to the bike details page\n",
    "                x_element.click()\n",
    "\n",
    "                # Additional scraping from the new page (e.g., bike name, price, etc.)\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.XPATH, '//h1')))\n",
    "                bike_name = driver.find_element(By.XPATH, '//h1').text\n",
    "                bike_price = driver.find_element(By.XPATH, '//span[@class=\"price\"]').text\n",
    "                \n",
    "                print(f\"Bike Name: {bike_name}, Price: {bike_price}\")\n",
    "\n",
    "                # Navigate back to the previous page\n",
    "                driver.execute_script(\"window.history.go(-1)\")\n",
    "                \n",
    "                # Wait for the bike list to load again\n",
    "                WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'bikelist')))\n",
    "                \n",
    "            except NoSuchElementException:\n",
    "                print(f\"No <h3> tag found for item {search_query}. Skipping...\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred while processing item {search_query}: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "    bikes_df_basicss = pd.DataFrame(bike_details, columns=['Bike URLs'])\n",
    "    # print(bikes_df_basicss)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")\n",
    "\n",
    "finally:\n",
    "    # Close the browser\n",
    "    if driver:\n",
    "        driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
